{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# import imageio\n",
    "from env.TFenv_masked import HangmanEnvironment\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent, DdqnAgent\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "\n",
    "\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from multiprocessing import Pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Observation Spec:\n{'observations': BoundedArraySpec(shape=(1, 30), dtype=dtype('float32'), name='game', minimum=-1.0, maximum=26.0), 'legal_moves': ArraySpec(shape=(26,), dtype=dtype('float32'), name=None)}\nReward Spec:\nArraySpec(shape=(), dtype=dtype('float32'), name='reward')\nAction Spec:\nBoundedArraySpec(shape=(), dtype=dtype('int32'), name='letter', minimum=0, maximum=25)\n"
     ]
    }
   ],
   "source": [
    "# Globals\n",
    "# NUMBER_EPOSODES = 250000\n",
    "NUMBER_EPOSODES = 1000000\n",
    "COLLECTION_STEPS = 1\n",
    "BATCH_SIZE = 64\n",
    "EVAL_EPISODES = 50\n",
    "EVAL_INTERVAL = 1000\n",
    "\n",
    "dictionary_path = \"../../../properties/words_250000_train.txt\"\n",
    "reward_map = {\n",
    "    \"game_success_reward\": 1.00,\n",
    "    \"lose_reward\": -0.5,\n",
    "    \"guess_success_reward\": 0.2,\n",
    "    \"guess_fail_reward\": -0.02,\n",
    "    \"guess_repeat_reward\": -5.,\n",
    "}\n",
    "life_initial = 15\n",
    "\n",
    "train_env = HangmanEnvironment(dictionary_path, reward_map,life_initial)\n",
    "evaluation_env = HangmanEnvironment(dictionary_path, reward_map,life_initial)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "\n",
    "print('Reward Spec:')\n",
    "print(train_env.time_step_spec().reward)\n",
    "\n",
    "print('Action Spec:')\n",
    "print(train_env.action_spec())\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "evaluation_env = tf_py_environment.TFPyEnvironment(evaluation_env)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_env.action_spec()\n",
    "train_env.time_step_spec()\n",
    "log_interval = 200\n",
    "def observation_and_action_constraint_splitter(obs):\n",
    "\treturn obs['observations'], np.logical_not(obs['legal_moves'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks = [[False,  True,  True,  True, False,  True,  True,  True,  True,\n",
    "#          True,  True, False,  True,  True,  True,  True,  True,  True,\n",
    "#          True,  True,  True,  True,  True,  True,  True,  True]]\n",
    "# tf.where(masks,0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_layers = (100,100,26)\n",
    "dropout_layers = (0.5, 0.5,0.2)\n",
    "\n",
    "# preprocessing_layers = {\n",
    "#     'observations': tf.keras.layers.Flatten(),\n",
    "#     'legal_moves': tf.keras.layers.Dense(26,)\n",
    "#     }\n",
    "# preprocessing_combiner = tf.keras.layers.Concatenate(axis=-1)\n",
    "# preprocessing_combiner =  tf.keras.layers.Add()\n",
    "\n",
    "\n",
    "# (100,)\n",
    "\n",
    "# dqn_network = QNetwork(\n",
    "#     train_env.observation_spec(),\n",
    "#     train_env.action_spec(),\n",
    "#     fc_layer_params=hidden_layers)\n",
    "\n",
    "# def observation_and_action_constrain_splitter(observation):\n",
    "#      action_mask = [0 for all lines]\n",
    "#      for each line in observation:\n",
    "#           if line.processed == 0:\n",
    "#                action_mask[line_number] = 1 #valid action\n",
    "#      return observation, tf.convert_to_tensor(action_mask, tf.dtype=np.int32)\n",
    "\n",
    "# categorical_q_network=categorical_q_network.CategoricalQNetwork(\n",
    "# \t\t\t\tenvironment.time_step_spec().observation['observations'],\n",
    "# \t\t\t\tenvironment.action_spec(),\n",
    "# \t\t\t\tnum_atoms=num_atoms,\n",
    "# \t\t\t\tfc_layer_params=fc_layer_params),\n",
    "    # train_env.observation_spec(),\n",
    "    # preprocessing_layers=preprocessing_layers,\n",
    "    # preprocessing_combiner=preprocessing_combiner,\n",
    "\n",
    "# environment.time_step_spec().observation['observations']\n",
    "ddqn_network = QNetwork(\n",
    "    train_env.time_step_spec().observation['observations'],\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=hidden_layers,\n",
    "    dropout_layer_params=None\n",
    "    )\n",
    "counter = tf.Variable(0)\n",
    "\n",
    "ddqn_agent = DdqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network = ddqn_network,\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3),\n",
    "    observation_and_action_constraint_splitter=observation_and_action_constraint_splitter,\n",
    "    td_errors_loss_fn = common.element_wise_squared_loss,\n",
    "    train_step_counter = counter)\n",
    "\n",
    "ddqn_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_average_reward(environment, policy, episodes=10):\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        while not time_step.is_last():\n",
    "        # for _ in range(4):\n",
    "            # print(time_step)\n",
    "            action_step = policy.action(time_step)\n",
    "            # print(action_step.action)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_reward += time_step.reward\n",
    "            # print(time_step.is_last())\n",
    "    \n",
    "        total_reward += episode_reward\n",
    "        avg_reward = total_reward / episodes\n",
    "    \n",
    "    return avg_reward.numpy()[0]\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, agent, enviroment):\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec=agent.collect_data_spec,\n",
    "            batch_size=enviroment.batch_size,\n",
    "            max_length=50000)\n",
    "        \n",
    "        self._random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                enviroment.action_spec(),\n",
    "                                                observation_and_action_constraint_splitter = observation_and_action_constraint_splitter)\n",
    "        \n",
    "        self._fill_buffer(train_env, self._random_policy, steps=100)\n",
    "        \n",
    "        self.dataset = self._replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3, \n",
    "            sample_batch_size=BATCH_SIZE, \n",
    "            num_steps=2).prefetch(3)\n",
    "\n",
    "        self.iterator = iter(self.dataset)\n",
    "    \n",
    "    def _fill_buffer(self, enviroment, policy, steps):\n",
    "        for _ in range(steps):\n",
    "            self.timestamp_data(enviroment, policy)\n",
    "            \n",
    "    def timestamp_data(self, environment, policy):\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        timestamp_trajectory = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "        self._replay_buffer.add_batch(timestamp_trajectory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(time_step.is_last())\n",
    "# action_step.action\n",
    "# time_step = evaluation_env.step(action_step.action)\n",
    "# time_step.reward\n",
    "# print(time_step.is_last())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# total_reward = 0.0\n",
    "# episodes=10\n",
    "\n",
    "# for _ in range(episodes):\n",
    "#     time_step = evaluation_env.reset()\n",
    "#     episode_reward = 0.0\n",
    "\n",
    "#     while not time_step.is_last():\n",
    "#     # for _ in range(4):\n",
    "#         print(time_step)\n",
    "#         action_step = ddqn_agent.policy.action(time_step)\n",
    "#         print(chr(int(action_step.action.numpy()) + 97))\n",
    "#         time_step = evaluation_env.step(action_step.action)\n",
    "#         print(\"\".join([chr(int(x) + 97) for x in time_step[3].numpy()[0][0]]))\n",
    "#         episode_reward += time_step.reward\n",
    "#         # print(time_step)\n",
    "\n",
    "#     total_reward += episode_reward\n",
    "#     avg_reward = total_reward / episodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**********************************\n",
      "Training DDQN\n",
      "**********************************\n",
      "You Found heraldical\n",
      "You Found unreformative\n",
      "before avg reward\n",
      "You Found wolftown\n",
      "You Found wastland\n",
      "You Found selfcongratulating\n",
      "You Found superimpregnated\n",
      "You Found unflamboyantly\n",
      "before iter\n",
      "before iter\n",
      "You Found intransformable\n",
      "500\n",
      "You Found demythologizer\n",
      "You Found phyton\n",
      "1000\n",
      "You Found decorrugative\n",
      "You Found downlinking\n",
      "You Found forcibleness\n",
      "You Found oldstanding\n",
      "You Found financial\n",
      "You Found grantham\n",
      "Episode 1000 - Average reward = 0.9580001831054688, Loss = 0.5460678339004517.\n",
      "You Found frailes\n",
      "You Found unsophisticate\n",
      "1500\n",
      "You Found glossary\n",
      "You Found selfproduction\n",
      "You Found coitions\n",
      "You Found psychagogy\n",
      "You Found godmaking\n",
      "You Found benzopyranyl\n",
      "2000\n",
      "You Found selfjudgment\n",
      "You Found unjeopardized\n",
      "You Found niton\n",
      "You Found witchwoman\n",
      "You Found lithia\n",
      "You Found precordiality\n",
      "Episode 2000 - Average reward = 0.9713334441184998, Loss = 1.028409481048584.\n",
      "You Found drygalski\n",
      "You Found klatch\n",
      "You Found lapcock\n",
      "2500\n",
      "You Found polynucleate\n",
      "You Found hypothalli\n",
      "3000\n",
      "You Found unsymptomatically\n",
      "You Found postbrachium\n",
      "You Found occipitoatlantal\n",
      "You Found shackings\n",
      "You Found witchingly\n",
      "Episode 3000 - Average reward = 0.7886667847633362, Loss = 0.2468765825033188.\n",
      "You Found hyperviscous\n",
      "You Found brandishers\n",
      "3500\n",
      "You Found worldsupplying\n",
      "You Found nonduplicative\n",
      "You Found malcontentism\n",
      "You Found crowvictuals\n",
      "You Found alkalization\n",
      "You Found lokaose\n",
      "You Found overcarelessness\n",
      "You Found stonishing\n",
      "You Found spondyloschisis\n",
      "4000\n",
      "You Found unparliamentary\n",
      "You Found cryptograms\n",
      "You Found overneglectfulness\n",
      "You Found whimming\n",
      "Episode 4000 - Average reward = 0.660666823387146, Loss = 0.19294308125972748.\n",
      "You Found antispastic\n",
      "You Found aphony\n",
      "You Found formulizing\n",
      "4500\n",
      "You Found stomatoplasty\n",
      "5000\n",
      "You Found nonmanifest\n",
      "You Found woodlawn\n",
      "You Found mandator\n",
      "Episode 5000 - Average reward = 0.5680000185966492, Loss = 0.4118342995643616.\n",
      "You Found subhemispherically\n",
      "You Found refamiliarized\n",
      "You Found gonof\n",
      "You Found plasticized\n",
      "5500\n",
      "You Found archdeaconate\n",
      "You Found fdic\n",
      "You Found gastroxynsis\n",
      "6000\n",
      "You Found toxicological\n",
      "You Found unmonkish\n",
      "You Found trainways\n",
      "You Found helicopters\n",
      "You Found overdiligently\n",
      "You Found burchfield\n",
      "Episode 6000 - Average reward = 0.8026667237281799, Loss = 0.4358431398868561.\n",
      "You Found vilenatured\n",
      "You Found snickdrawing\n",
      "6500\n",
      "You Found plasmocytoma\n",
      "You Found sulphonamic\n",
      "You Found sacralgia\n",
      "You Found cacophonia\n",
      "7000\n",
      "You Found rubbercollecting\n",
      "You Found influenceabilities\n",
      "You Found strobilophyta\n",
      "Episode 7000 - Average reward = 0.5586667656898499, Loss = 0.24208727478981018.\n",
      "You Found katabasis\n",
      "You Found palatist\n",
      "You Found springwurzel\n",
      "You Found flavoring\n",
      "You Found parricidally\n",
      "You Found blackbrush\n",
      "You Found selfimpedance\n",
      "7500\n",
      "You Found twiceconfirmed\n",
      "You Found franklinville\n",
      "You Found kilaya\n",
      "You Found sciagraphy\n",
      "You Found phagedenic\n",
      "You Found ultrahighfrequency\n",
      "8000\n",
      "You Found yachtsman\n",
      "You Found prionodon\n",
      "You Found disequilibration\n",
      "You Found adipic\n",
      "Episode 8000 - Average reward = 0.6340001225471497, Loss = 1.0411652326583862.\n",
      "You Found nomotheism\n",
      "8500\n",
      "You Found contemplatively\n",
      "You Found blepharelcosis\n",
      "9000\n",
      "You Found unflappability\n",
      "You Found anoplotheroid\n",
      "You Found laos\n",
      "You Found artsy\n",
      "You Found palsystricken\n",
      "You Found whalan\n",
      "You Found pranidhana\n",
      "You Found forecasting\n",
      "Episode 9000 - Average reward = 1.1680001020431519, Loss = 0.2931438088417053.\n",
      "You Found unpaid\n",
      "You Found phenylcarbamic\n",
      "9500\n",
      "You Found lymphadenoma\n",
      "You Found circumstantially\n",
      "You Found sifflot\n",
      "10000\n",
      "You Found chawstick\n",
      "Episode 10000 - Average reward = 0.36466673016548157, Loss = 0.11858724057674408.\n",
      "You Found radiotechnology\n",
      "You Found protomammalian\n",
      "You Found glonoins\n",
      "You Found antilacrosse\n",
      "You Found kodiak\n",
      "10500\n",
      "You Found hydrochlorothiazide\n",
      "You Found producibleness\n",
      "You Found kistiakowsky\n",
      "11000\n",
      "You Found unbondableness\n",
      "You Found nontragicalness\n",
      "You Found cystorrhaphy\n",
      "You Found doornail\n",
      "You Found alimentotherapy\n",
      "Episode 11000 - Average reward = 0.8520001173019409, Loss = 0.18664491176605225.\n",
      "You Found transmittible\n",
      "You Found mightyhearted\n",
      "You Found aja\n",
      "You Found counterreplies\n",
      "You Found alacrity\n",
      "You Found wadis\n",
      "You Found djailolo\n",
      "11500\n",
      "You Found considerations\n",
      "You Found plasmodesm\n",
      "You Found straightforwardness\n",
      "You Found pseudobrachial\n",
      "You Found diacaustic\n",
      "12000\n",
      "You Found antiglyoxalase\n",
      "You Found autolyzing\n",
      "Episode 12000 - Average reward = 0.5133334994316101, Loss = 0.3844227194786072.\n",
      "You Found onaga\n",
      "You Found unconspicuously\n",
      "You Found windowrattling\n",
      "You Found noncostraight\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-1041890279c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m# print(f\"ddqn_agent: {_}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mddqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\agents\\tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, experience, weights, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m       loss_info = self._train_fn(\n\u001b[0m\u001b[0;32m    519\u001b[0m           experience=experience, weights=weights, **kwargs)\n\u001b[0;32m    520\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\utils\\common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\agents\\dqn\\dqn_agent.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, experience, weights)\u001b[0m\n\u001b[0;32m    385\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m       loss_info = self._loss(\n\u001b[0m\u001b[0;32m    388\u001b[0m           \u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m           \u001b[0mtd_errors_loss_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_td_errors_loss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\agents\\dqn\\dqn_agent.py\u001b[0m in \u001b[0;36m_loss\u001b[1;34m(self, experience, td_errors_loss_fn, gamma, reward_scale_factor, weights, training)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m       \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m       next_q_values = self._compute_next_q_values(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\agents\\dqn\\dqn_agent.py\u001b[0m in \u001b[0;36m_compute_q_values\u001b[1;34m(self, time_steps, actions, training)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_observation_and_action_constraint_splitter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m       network_observation, _ = self._observation_and_action_constraint_splitter(\n\u001b[0m\u001b[0;32m    529\u001b[0m           network_observation)\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-d64b7ba66c12>\u001b[0m in \u001b[0;36mobservation_and_action_constraint_splitter\u001b[1;34m(obs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mobservation_and_action_constraint_splitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'observations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'legal_moves'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"**********************************\")\n",
    "print(\"Training DDQN\")\n",
    "print(\"**********************************\")\n",
    "\n",
    "# def train(agent):\n",
    "experience_replay = ExperienceReplay(ddqn_agent, train_env)\n",
    "ddqn_agent.train_step_counter.assign(0)\n",
    "print(\"before avg reward\")\n",
    "\n",
    "avg_reward = get_average_reward(evaluation_env, ddqn_agent.policy, EVAL_EPISODES)\n",
    "print(\"before iter\")\n",
    "rewards = [avg_reward]\n",
    "print(\"before iter\")\n",
    "\n",
    "for _ in range(NUMBER_EPOSODES):\n",
    "    for _ in range(COLLECTION_STEPS):\n",
    "        experience_replay.timestamp_data(train_env, ddqn_agent.collect_policy)\n",
    "\n",
    "    experience, info = next(experience_replay.iterator)\n",
    "    train_loss = ddqn_agent.train(experience).loss\n",
    "    # print(f\"ddqn_agent: {_}\")\n",
    "    if ddqn_agent.train_step_counter.numpy() % 500 == 0:\n",
    "        print(ddqn_agent.train_step_counter.numpy())\n",
    "    if ddqn_agent.train_step_counter.numpy() % EVAL_INTERVAL == 0:\n",
    "        avg_reward = get_average_reward(evaluation_env, ddqn_agent.policy, EVAL_EPISODES)\n",
    "        print('Episode {0} - Average reward = {1}, Loss = {2}.'.format(\n",
    "                ddqn_agent.train_step_counter.numpy(), avg_reward, train_loss))\n",
    "        rewards.append(avg_reward)\n",
    "            \n",
    "    # return rewards, agent, experience_replay\n",
    "\n",
    "# print(\"**********************************\")\n",
    "# print(\"Training DQN\")\n",
    "# print(\"**********************************\")\n",
    "# dqn_reward = train(dqn_agent)\n",
    "\n",
    "print(\"**********************************\")\n",
    "print(\"DONE\")\n",
    "print(\"**********************************\")\n",
    "# ddqn_reward, agent = train(ddqn_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# # train_env.render()\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "ddqn_agent.train_step_counter.numpy()\n",
    "# time_step = train_env.current_time_step()\n",
    "# print(time_step)\n",
    "# print(\"\".join([chr(int(x)+97) for x in time_step[3][0][0]]).replace(\"{\",\".\").replace('`',\"\"))\n",
    "\n",
    "# action_step = ddqn_agent.policy.action(time_step)\n",
    "# action_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (68,)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5fde860611f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m68\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVAL_INTERVAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DDQN Rewards'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Average reward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2838\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2839\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2840\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2841\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1741\u001b[0m         \"\"\"\n\u001b[0;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (68,)"
     ]
    }
   ],
   "source": [
    "episodes = range(0, NUMBER_EPOSODES + 1, EVAL_INTERVAL)\n",
    "# episodes = range(0, 68, EVAL_INTERVAL)\n",
    "\n",
    "plt.plot(episodes, rewards, label = 'DDQN Rewards')\n",
    "plt.legend()\n",
    "plt.ylabel('Average reward')\n",
    "plt.xlabel('Epoisodes')\n",
    "plt.ylim(top=3, bottom=-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.tensorflow.org/agents/tutorials/3_policies_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".23# # save policy\n",
    "# PolicySaver(eval_policy).save(model_dir + 'eval_policy')\n",
    "# ...\n",
    "# # load policy\n",
    "# policy = tf.saved_model.load(model_path)\n",
    "# action = policy.action(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())\n",
    "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=ExperienceReplay,\n",
    ")\n",
    "train_checkpointer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dir = os.path.join(tempdir, 'policy')\n",
    "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
    "tf_policy_saver.save(policy_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmg",
   "language": "python",
   "name": "hmg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}