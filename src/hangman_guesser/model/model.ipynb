{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "hangman_guesser",
   "display_name": "Python 3.8.5 64-bit ('hangman-guesser-MyuQKO45-py3.8': poetry)"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# import imageio\n",
    "from hangman_guesser.env.TFenv import HangmanEnvironment\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent, DdqnAgent\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "\n",
    "\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from multiprocessing import Pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Observation Spec:\nBoundedArraySpec(shape=(1, 30), dtype=dtype('int32'), name='game', minimum=-1, maximum=26)\nReward Spec:\nArraySpec(shape=(), dtype=dtype('float32'), name='reward')\nAction Spec:\nBoundedArraySpec(shape=(), dtype=dtype('int32'), name='letter', minimum=0, maximum=25)\n"
     ]
    }
   ],
   "source": [
    "# Globals\n",
    "NUMBER_EPOSODES = 1000\n",
    "COLLECTION_STEPS = 1\n",
    "BATCH_SIZE = 64\n",
    "EVAL_EPISODES = 10\n",
    "EVAL_INTERVAL = 50\n",
    "\n",
    "dictionary_path = \"../../../properties/words_250000_train.txt\"\n",
    "train_env = HangmanEnvironment(dictionary_path)\n",
    "evaluation_env = HangmanEnvironment(dictionary_path)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "\n",
    "print('Reward Spec:')\n",
    "print(train_env.time_step_spec().reward)\n",
    "\n",
    "print('Action Spec:')\n",
    "print(train_env.action_spec())\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "evaluation_env = tf_py_environment.TFPyEnvironment(evaluation_env)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(1, 30), dtype=tf.int32, name='game', minimum=array(-1, dtype=int32), maximum=array(26, dtype=int32)))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# train_env.action_spec()\n",
    "train_env.time_step_spec()\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_layers = (100,)\n",
    "\n",
    "# dqn_network = QNetwork(\n",
    "#     train_env.observation_spec(),\n",
    "#     train_env.action_spec(),\n",
    "#     fc_layer_params=hidden_layers)\n",
    "\n",
    "ddqn_network = QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=hidden_layers)\n",
    "\n",
    "counter = tf.Variable(0)\n",
    "\n",
    "ddqn_agent = DdqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network = ddqn_network,\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3),\n",
    "    td_errors_loss_fn = common.element_wise_squared_loss,\n",
    "    train_step_counter = counter)\n",
    "\n",
    "ddqn_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_average_reward(environment, policy, episodes=10):\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_reward += time_step.reward\n",
    "            print(time_step)\n",
    "    \n",
    "        total_reward += episode_reward\n",
    "        avg_reward = total_reward / episodes\n",
    "    \n",
    "    return avg_reward.numpy()[0]\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, agent, enviroment):\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec=agent.collect_data_spec,\n",
    "            batch_size=enviroment.batch_size,\n",
    "            max_length=50000)\n",
    "        \n",
    "        self._random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                enviroment.action_spec())\n",
    "        \n",
    "        self._fill_buffer(train_env, self._random_policy, steps=100)\n",
    "        \n",
    "        self.dataset = self._replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3, \n",
    "            sample_batch_size=BATCH_SIZE, \n",
    "            num_steps=2).prefetch(3)\n",
    "\n",
    "        self.iterator = iter(self.dataset)\n",
    "    \n",
    "    def _fill_buffer(self, enviroment, policy, steps):\n",
    "        for _ in range(steps):\n",
    "            self.timestamp_data(enviroment, policy)\n",
    "            \n",
    "    def timestamp_data(self, environment, policy):\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        timestamp_trajectory = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "        self._replay_buffer.add_batch(timestamp_trajectory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(agent):\n",
    "print(\"hello\")\n",
    "experience_replay = ExperienceReplay(ddqn_agent, train_env)\n",
    "print(\"hello\")\n",
    "\n",
    "ddqn_agent.train_step_counter.assign(0)\n",
    "print(\"hello\")\n",
    "\n",
    "avg_reward = get_average_reward(evaluation_env, ddqn_agent.policy, EVAL_EPISODES)\n",
    "rewards = [avg_reward]\n",
    "print(\"hello\")\n",
    "\n",
    "for _ in range(NUMBER_EPOSODES):\n",
    "    print(\"hello\")\n",
    "    for _ in range(COLLECTION_STEPS):\n",
    "        experience_replay.timestamp_data(train_env, ddqn_agent.collect_policy)\n",
    "\n",
    "    experience, info = next(experience_replay.iterator)\n",
    "    train_loss = ddqn_agent.train(experience).loss\n",
    "    print(\"hello\")\n",
    "\n",
    "    print(ddqn_agent.train_step_counter.numpy())\n",
    "    if ddqn_agent.train_step_counter.numpy() % EVAL_INTERVAL == 0:\n",
    "        avg_reward = get_average_reward(evaluation_env, ddqn_agent.policy, EVAL_EPISODES)\n",
    "        print('Episode {0} - Average reward = {1}, Loss = {2}.'.format(\n",
    "                ddqn_agent.train_step_counter.numpy(), avg_reward, train_loss))\n",
    "        rewards.append(avg_reward)\n",
    "            \n",
    "    # return rewards, agent, experience_replay\n",
    "\n",
    "# print(\"**********************************\")\n",
    "# print(\"Training DQN\")\n",
    "# print(\"**********************************\")\n",
    "# dqn_reward = train(dqn_agent)\n",
    "\n",
    "print(\"**********************************\")\n",
    "print(\"Training DDQN\")\n",
    "print(\"**********************************\")\n",
    "# ddqn_reward, agent = train(ddqn_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# # train_env.render()\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "ddqn_agent.train_step_counter.numpy()\n",
    "# time_step = train_env.current_time_step()\n",
    "# print(time_step)\n",
    "# print(\"\".join([chr(int(x)+97) for x in time_step[3][0][0]]).replace(\"{\",\".\").replace('`',\"\"))\n",
    "\n",
    "# action_step = ddqn_agent.policy.action(time_step)\n",
    "# action_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = range(0, NUMBER_EPOSODES + 1, EVAL_INTERVAL)\n",
    "plt.plot(episodes, ddqn_reward, label = 'DDQN Rewards')\n",
    "plt.legend()\n",
    "plt.ylabel('Average reward')\n",
    "plt.xlabel('Epoisodes')\n",
    "plt.ylim(top=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save policy\n",
    "# PolicySaver(eval_policy).save(model_dir + 'eval_policy')\n",
    "# ...\n",
    "# # load policy\n",
    "# policy = tf.saved_model.load(model_path)\n",
    "# action = policy.action(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())\n",
    "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=ExperienceReplay,\n",
    ")\n",
    "train_checkpointer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dir = os.path.join(tempdir, 'policy')\n",
    "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
    "tf_policy_saver.save(policy_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
