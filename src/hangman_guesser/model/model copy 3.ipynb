{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# import imageio\n",
    "from env.TFenv import HangmanEnvironment\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent, DdqnAgent\n",
    "from tf_agents.networks.q_rnn_network import QRnnNetwork\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "\n",
    "\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from multiprocessing import Pool\n",
    "import functools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Observation Spec:\nBoundedArraySpec(shape=(1, 30), dtype=dtype('int32'), name='game', minimum=-1, maximum=26)\nReward Spec:\nArraySpec(shape=(), dtype=dtype('float32'), name='reward')\nAction Spec:\nBoundedArraySpec(shape=(), dtype=dtype('int32'), name='letter', minimum=0, maximum=25)\n"
     ]
    }
   ],
   "source": [
    "# Globals\n",
    "NUMBER_EPOSODES = 250000\n",
    "COLLECTION_STEPS = 1\n",
    "BATCH_SIZE = 64\n",
    "EVAL_EPISODES = 30\n",
    "EVAL_INTERVAL = 1000\n",
    "\n",
    "dictionary_path = \"../../../properties/words_250000_train.txt\"\n",
    "reward_map = {\n",
    "    \"game_success_reward\": 1.00,\n",
    "    \"lose_reward\": -0.,\n",
    "    \"guess_success_reward\": 0.2,\n",
    "    \"guess_fail_reward\": -0.,\n",
    "    \"guess_repeat_reward\": -5.,\n",
    "}\n",
    "life_initial = 15\n",
    "\n",
    "train_env = HangmanEnvironment(dictionary_path, reward_map,life_initial)\n",
    "evaluation_env = HangmanEnvironment(dictionary_path, reward_map,life_initial)\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "\n",
    "print('Reward Spec:')\n",
    "print(train_env.time_step_spec().reward)\n",
    "\n",
    "print('Action Spec:')\n",
    "print(train_env.action_spec())\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "evaluation_env = tf_py_environment.TFPyEnvironment(evaluation_env)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_env.action_spec()\n",
    "train_env.time_step_spec()\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.keras_layers import dynamic_unroll_layer\n",
    "from tf_agents.networks import sequential\n",
    "KERAS_LSTM_FUSED = 2\n",
    "\n",
    "\n",
    "\n",
    "train_sequence_length = life_initial\n",
    "action_spec = train_env.action_spec()\n",
    "num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "input_fc_layer_params=(50,)\n",
    "lstm_size=(20,)\n",
    "output_fc_layer_params=(20,)\n",
    "# logits = functools.partial(\n",
    "#     tf.keras.layers.Dense,\n",
    "#     activation=None,\n",
    "#     kernel_initializer=tf.random_uniform_initializer(minval=-0.03, maxval=0.03),\n",
    "#     bias_initializer=tf.constant_initializer(-0.2))\n",
    "\n",
    "\n",
    "# dense = functools.partial(\n",
    "#     tf.keras.layers.Dense,\n",
    "#     activation=tf.keras.activations.relu,\n",
    "#     kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "#         scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "\n",
    "# fused_lstm_cell = functools.partial(\n",
    "#     tf.keras.layers.LSTMCell, implementation=KERAS_LSTM_FUSED)\n",
    "\n",
    "\n",
    "# def create_recurrent_network(\n",
    "#     input_fc_layer_units,\n",
    "#     lstm_size,\n",
    "#     output_fc_layer_units,\n",
    "#     num_actions):\n",
    "#   rnn_cell = tf.keras.layers.StackedRNNCells(\n",
    "#       [fused_lstm_cell(s) for s in lstm_size])\n",
    "#   return sequential.Sequential(\n",
    "#       [dense(num_units) for num_units in input_fc_layer_units]\n",
    "#       + [dynamic_unroll_layer.DynamicUnroll(rnn_cell)]\n",
    "#       + [dense(num_units) for num_units in output_fc_layer_units]\n",
    "#       + [logits(num_actions)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_layers = (512, 3)\n",
    "# (100,)\n",
    "\n",
    "# dqn_network = QNetwork(\n",
    "#     train_env.observation_spec(),\n",
    "#     train_env.action_spec(),\n",
    "#     fc_layer_params=hidden_layers)\n",
    "\n",
    "\n",
    "preprocessing_combiner = tf.keras.layers.Concatenate(axis=-1)\n",
    "preprocessing_layers = tf.keras.layers.Flatten()\n",
    "\n",
    "ddqn_network = QRnnNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layers,\n",
    "    input_fc_layer_params=hidden_layers,\n",
    "    lstm_size =lstm_size ,\n",
    "    output_fc_layer_params=output_fc_layer_params\n",
    "    )\n",
    "# ddqn_network = QRnnNetwork\n",
    "\n",
    "# ddqn_network = create_recurrent_network(\n",
    "#           input_fc_layer_params,\n",
    "#           lstm_size,\n",
    "#           output_fc_layer_params,\n",
    "#           num_actions)\n",
    "\n",
    "counter = tf.Variable(0)\n",
    "\n",
    "\n",
    "ddqn_agent = DdqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network = ddqn_network,\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3),\n",
    "    td_errors_loss_fn = common.element_wise_squared_loss,\n",
    "    # n_step_update=num_actions,\n",
    "    train_step_counter = counter)\n",
    "\n",
    "ddqn_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_average_reward(environment, policy, episodes=10):\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "        # for _ in range(4):\n",
    "            action_step = policy.action(time_step)\n",
    "            # print(action_step.action)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_reward += time_step.reward\n",
    "            # print(time_step.is_last())\n",
    "    \n",
    "        total_reward += episode_reward\n",
    "        avg_reward = total_reward / episodes\n",
    "    \n",
    "    return avg_reward.numpy()[0]\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, agent, enviroment):\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec=agent.collect_data_spec,\n",
    "            batch_size=enviroment.batch_size,\n",
    "            max_length=50000)\n",
    "        \n",
    "        self._random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                enviroment.action_spec())\n",
    "        \n",
    "        self._fill_buffer(train_env, self._random_policy, steps=100)\n",
    "        \n",
    "        self.dataset = self._replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3, \n",
    "            sample_batch_size=BATCH_SIZE, \n",
    "            num_steps=train_sequence_length + 1).prefetch(3)\n",
    "\n",
    "        self.iterator = iter(self.dataset)\n",
    "    \n",
    "    def _fill_buffer(self, enviroment, policy, steps):\n",
    "        for _ in range(steps):\n",
    "            self.timestamp_data(enviroment, policy)\n",
    "            \n",
    "    def timestamp_data(self, environment, policy):\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        timestamp_trajectory = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "        self._replay_buffer.add_batch(timestamp_trajectory)\n",
    "\n",
    "    def collect_driver(self,enviroment):\n",
    "        collect_policy = tf_agent.collect_policy\n",
    "        collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "            tf_env,\n",
    "            collect_policy,\n",
    "            observers=[replay_buffer.add_batch] + train_metrics,\n",
    "            num_steps=collect_steps_per_iteration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(time_step.is_last())\n",
    "# action_step.action\n",
    "# time_step = evaluation_env.step(action_step.action)\n",
    "# time_step.reward\n",
    "# print(time_step.is_last())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "policy_state and policy_state_spec structures do not match:\n  ()\nvs.\n  ListWrapper([., .])",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-16009404edab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# for _ in range(4):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m97\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\utils\\common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \"\"\"\n\u001b[0;32m    559\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tf_agents_tf_policy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m     \u001b[0mdistribution_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[0;32m    562\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\policies\\greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m     78\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mDeterministicWithLogProb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0m\u001b[0;32m     81\u001b[0m         time_step, policy_state)\n\u001b[0;32m     82\u001b[0m     return policy_step.PolicyStep(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    395\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           message='time_step and time_step_spec structures do not match')\n\u001b[1;32m--> 397\u001b[1;33m       nest_utils.assert_same_structure(\n\u001b[0m\u001b[0;32m    398\u001b[0m           \u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_policy_state_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites, message)\u001b[0m\n\u001b[0;32m    112\u001b[0m     str2 = tf.nest.map_structure(\n\u001b[0;32m    113\u001b[0m         lambda _: _DOT, nest2, expand_composites=expand_composites)\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}:\\n  {}\\nvs.\\n  {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: policy_state and policy_state_spec structures do not match:\n  ()\nvs.\n  ListWrapper([., .])"
     ]
    }
   ],
   "source": [
    "\n",
    "total_reward = 0.0\n",
    "episodes=10\n",
    "\n",
    "for _ in range(episodes):\n",
    "    time_step = evaluation_env.reset()\n",
    "    episode_reward = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "    # for _ in range(4):\n",
    "        action_step = ddqn_agent.policy.action(time_step)\n",
    "        print(chr(int(action_step.action.numpy()) + 97))\n",
    "        time_step = evaluation_env.step(action_step.action)\n",
    "        print(\"\".join([chr(int(x) + 97) for x in time_step[3].numpy()[0][0]]))\n",
    "        episode_reward += time_step.reward\n",
    "        # print(time_step)\n",
    "\n",
    "    total_reward += episode_reward\n",
    "    avg_reward = total_reward / episodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**********************************\n",
      "Training DDQN\n",
      "**********************************\n",
      "before avg reward\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "policy_state and policy_state_spec structures do not match:\n  ()\nvs.\n  ListWrapper([., .])",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-1041890279c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"before avg reward\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mavg_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_average_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluation_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVAL_EPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"before iter\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mavg_reward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-c6ef1541bced>\u001b[0m in \u001b[0;36mget_average_reward\u001b[1;34m(environment, policy, episodes)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# for _ in range(4):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[1;31m# print(action_step.action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\utils\\common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \"\"\"\n\u001b[0;32m    559\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tf_agents_tf_policy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m     \u001b[0mdistribution_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[0;32m    562\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\policies\\greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m     78\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mDeterministicWithLogProb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0m\u001b[0;32m     81\u001b[0m         time_step, policy_state)\n\u001b[0;32m     82\u001b[0m     return policy_step.PolicyStep(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    395\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           message='time_step and time_step_spec structures do not match')\n\u001b[1;32m--> 397\u001b[1;33m       nest_utils.assert_same_structure(\n\u001b[0m\u001b[0;32m    398\u001b[0m           \u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_policy_state_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\hmg\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites, message)\u001b[0m\n\u001b[0;32m    112\u001b[0m     str2 = tf.nest.map_structure(\n\u001b[0;32m    113\u001b[0m         lambda _: _DOT, nest2, expand_composites=expand_composites)\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}:\\n  {}\\nvs.\\n  {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: policy_state and policy_state_spec structures do not match:\n  ()\nvs.\n  ListWrapper([., .])"
     ]
    }
   ],
   "source": [
    "print(\"**********************************\")\n",
    "print(\"Training DDQN\")\n",
    "print(\"**********************************\")\n",
    "\n",
    "# def train(agent):\n",
    "experience_replay = ExperienceReplay(ddqn_agent, train_env)\n",
    "ddqn_agent.train_step_counter.assign(0)\n",
    "print(\"before avg reward\")\n",
    "\n",
    "avg_reward = get_average_reward(evaluation_env, ddqn_agent.policy, EVAL_EPISODES)\n",
    "print(\"before iter\")\n",
    "rewards = [avg_reward]\n",
    "print(\"before iter\")\n",
    "\n",
    "for _ in range(NUMBER_EPOSODES):\n",
    "    for _ in range(COLLECTION_STEPS):\n",
    "        experience_replay.timestamp_data(train_env, ddqn_agent.collect_policy)\n",
    "\n",
    "    experience, info = next(experience_replay.iterator)\n",
    "    train_loss = ddqn_agent.train(experience).loss\n",
    "    # print(f\"ddqn_agent: {_}\")\n",
    "    if ddqn_agent.train_step_counter.numpy() % 500 == 0:\n",
    "        print(ddqn_agent.train_step_counter.numpy())\n",
    "    if ddqn_agent.train_step_counter.numpy() % EVAL_INTERVAL == 0:\n",
    "        avg_reward = get_average_reward(evaluation_env, ddqn_agent.policy, EVAL_EPISODES)\n",
    "        print('Episode {0} - Average reward = {1}, Loss = {2}.'.format(\n",
    "                ddqn_agent.train_step_counter.numpy(), avg_reward, train_loss))\n",
    "        rewards.append(avg_reward)\n",
    "            \n",
    "    # return rewards, agent, experience_replay\n",
    "\n",
    "# print(\"**********************************\")\n",
    "# print(\"Training DQN\")\n",
    "# print(\"**********************************\")\n",
    "# dqn_reward = train(dqn_agent)\n",
    "\n",
    "print(\"**********************************\")\n",
    "print(\"DONE\")\n",
    "print(\"**********************************\")\n",
    "# ddqn_reward, agent = train(ddqn_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_env.render()\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "ddqn_agent.train_step_counter.numpy()\n",
    "# time_step = train_env.current_time_step()\n",
    "# print(time_step)\n",
    "# print(\"\".join([chr(int(x)+97) for x in time_step[3][0][0]]).replace(\"{\",\".\").replace('`',\"\"))\n",
    "\n",
    "# action_step = ddqn_agent.policy.action(time_step)\n",
    "# action_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = range(0, NUMBER_EPOSODES + 1, EVAL_INTERVAL)\n",
    "plt.plot(episodes, rewards, label = 'DDQN Rewards')\n",
    "plt.legend()\n",
    "plt.ylabel('Average reward')\n",
    "plt.xlabel('Epoisodes')\n",
    "plt.ylim(top=1, bottom=-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.tensorflow.org/agents/tutorials/3_policies_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".23# # save policy\n",
    "# PolicySaver(eval_policy).save(model_dir + 'eval_policy')\n",
    "# ...\n",
    "# # load policy\n",
    "# policy = tf.saved_model.load(model_path)\n",
    "# action = policy.action(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())\n",
    "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=ExperienceReplay,\n",
    ")\n",
    "train_checkpointer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dir = os.path.join(tempdir, 'policy')\n",
    "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
    "tf_policy_saver.save(policy_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmg",
   "language": "python",
   "name": "hmg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}